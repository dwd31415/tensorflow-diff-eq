{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to tensorflow-diff-eq\n\n\nA python package for simulating differential equations using \nTensorFlow\n.\n\n\n\n\nWhy TensorFlow?\n\n\nTensorFlow\n is an open source library for numerical computation(mostly in machine learning), that allows one to specify computations using abstract dataflow graphs. The library then runs them on the GPU or CPU, this makes TensorFlow great for simulating differential equations. GPUs can perform many computation much faster than CPUs, using this library one can harness this performance for simulating differential equations. The simulation can run on the GPU or the CPU without any change in code.\n\n\nInstallation\n\n\nThis package is still in an early stage of development, therefore it is not yet\navailable from PyPI. To install it, just clone this repository and run:\n\n\npython3 setup.py install\n\n\n\n\n\nBasic Usage\n\n\nThis package allows you to symbolically define differential equation and\nsimulate them using TensorFlow, this allows one to use the GPU for the actual simulation without ever using C\nor CUDA. All important classes and functions are in the \ndiff_eq\n namespace, therefore\nthis is best way to import tensorflow-diff-eq:\n\n\nimport\n \ntensorflow_diff_eq.diff_eq\n \nas\n \ndiff_eq\n\n\n\n\n\n\nThe first step in defining a differential equation is to create a \nDifferentialEquation\n\nobject:\n\n\neq\n \n=\n \ndiff_eq\n.\nDifferentialEquation\n()\n\n\n\n\n\n\nThen you must specify how many quantities will be involved in your differential equation.\n\n\nExample:\n\n\n(\nx\n,\n \nk\n,\n \nm\n)\n \n=\n \neq\n.\ngenerate_quantities\n(\n3\n)\n\n\n\n\n\n\nOnce you have the references for these quantities you can define them using the\n\ndefine_quantity\n function.\n\n\nExample:\n\n\neq\n.\ndefine_quantity\n(\nk\n,\n \n3.0\n,\n \n0\n,\n \n1\n)(\n0.0\n)\n\n\neq\n.\ndefine_quantity\n(\nm\n,\n \n0.7\n,\n \n0\n,\n \n1\n)(\n0.0\n)\n\n\neq\n.\ndefine_quantity\n(\nx\n,\n \n1.0\n,\n \n1\n,\n \n2\n)(\n-\nk\n.\nd\n(\n0\n)\n*\nx\n.\nd\n(\n0\n)\n/\nm\n.\nd\n(\n0\n))\n\n\n\n\n\n\nThe first argument to this function is the quantity which is being defined, the second parameter is an initial value and the third parameter is the grade of the derivative, that shall be\ninitialized with it. All other derivatives are initialized to zero of the respective datatype.\nThe forth argument is the grade of the \"definition\" derivative, this definition has the form\n\n\\frac{d^nx}{dt^n} = \\phi,\n\nwhere \nn\n is the grade supplied as the forth argument and \n\\phi\n is any expression\ninvolving previously defined quantities or \nx\n, which is the quantity that is currently being defined. Arbitrary \nn^{th}\n-grade derivatives of the quantities are accessed using \n.d(n)\n, the quantity itself has to be accessed using \n.d(0)\n.\n\n\nOnce you have supplied the system with the differential equation in this fashion, you can start the simulation. In order to do this, you must generate a tensorflow operation that runs\nthe simulation. Such an operation can be generated using \neq.generate_simulate_operation\n and can then be run using a tensorflow session. The function has one argument which is the \n\\Delta t\n that will be used for the simulation.\n\n\n\n\nWarning\n\n\nPlease make sure you run the \ntf.initialize_all_variables()\n operation after defining the quantities involved in your differential equation.\n\n\n\n\nExample:\n\n\n# Start tensorflow\n\n\nsess\n \n=\n \ntf\n.\nSession\n()\n\n\nsess\n.\nrun\n(\ntf\n.\ninitialize_all_variables\n())\n\n\nsimulate_op\n \n=\n \neq\n.\ngenerate_simulate_operation\n(\n0.005\n)\n\n\n\n\n\n\nAt this point you can just do \nsess.run(simulate_op)\n to simulate, if you want to access\nany quantity at any time you can run \nsess.run(x.d(0))\n, where \nx\n is the quantity that you want to know the value of.\n\n\nIf you wish want to know more about how this package works, have a look at the example it the \nexamples folder\n.\n\n\n\n\nTip\n\n\nIt is quite handy to define a \nt\n quantity and set it's first derivative to one in order to keep track of the \"time\". The library does not do this for you.",
            "title": "Home"
        },
        {
            "location": "/#welcome-to-tensorflow-diff-eq",
            "text": "A python package for simulating differential equations using  TensorFlow .",
            "title": "Welcome to tensorflow-diff-eq"
        },
        {
            "location": "/#why-tensorflow",
            "text": "TensorFlow  is an open source library for numerical computation(mostly in machine learning), that allows one to specify computations using abstract dataflow graphs. The library then runs them on the GPU or CPU, this makes TensorFlow great for simulating differential equations. GPUs can perform many computation much faster than CPUs, using this library one can harness this performance for simulating differential equations. The simulation can run on the GPU or the CPU without any change in code.",
            "title": "Why TensorFlow?"
        },
        {
            "location": "/#installation",
            "text": "This package is still in an early stage of development, therefore it is not yet\navailable from PyPI. To install it, just clone this repository and run:  python3 setup.py install",
            "title": "Installation"
        },
        {
            "location": "/#basic-usage",
            "text": "This package allows you to symbolically define differential equation and\nsimulate them using TensorFlow, this allows one to use the GPU for the actual simulation without ever using C\nor CUDA. All important classes and functions are in the  diff_eq  namespace, therefore\nthis is best way to import tensorflow-diff-eq:  import   tensorflow_diff_eq.diff_eq   as   diff_eq   The first step in defining a differential equation is to create a  DifferentialEquation \nobject:  eq   =   diff_eq . DifferentialEquation ()   Then you must specify how many quantities will be involved in your differential equation.",
            "title": "Basic Usage"
        },
        {
            "location": "/#example",
            "text": "( x ,   k ,   m )   =   eq . generate_quantities ( 3 )   Once you have the references for these quantities you can define them using the define_quantity  function.",
            "title": "Example:"
        },
        {
            "location": "/#example_1",
            "text": "eq . define_quantity ( k ,   3.0 ,   0 ,   1 )( 0.0 )  eq . define_quantity ( m ,   0.7 ,   0 ,   1 )( 0.0 )  eq . define_quantity ( x ,   1.0 ,   1 ,   2 )( - k . d ( 0 ) * x . d ( 0 ) / m . d ( 0 ))   The first argument to this function is the quantity which is being defined, the second parameter is an initial value and the third parameter is the grade of the derivative, that shall be\ninitialized with it. All other derivatives are initialized to zero of the respective datatype.\nThe forth argument is the grade of the \"definition\" derivative, this definition has the form \\frac{d^nx}{dt^n} = \\phi, \nwhere  n  is the grade supplied as the forth argument and  \\phi  is any expression\ninvolving previously defined quantities or  x , which is the quantity that is currently being defined. Arbitrary  n^{th} -grade derivatives of the quantities are accessed using  .d(n) , the quantity itself has to be accessed using  .d(0) .  Once you have supplied the system with the differential equation in this fashion, you can start the simulation. In order to do this, you must generate a tensorflow operation that runs\nthe simulation. Such an operation can be generated using  eq.generate_simulate_operation  and can then be run using a tensorflow session. The function has one argument which is the  \\Delta t  that will be used for the simulation.   Warning  Please make sure you run the  tf.initialize_all_variables()  operation after defining the quantities involved in your differential equation.",
            "title": "Example:"
        },
        {
            "location": "/#example_2",
            "text": "# Start tensorflow  sess   =   tf . Session ()  sess . run ( tf . initialize_all_variables ())  simulate_op   =   eq . generate_simulate_operation ( 0.005 )   At this point you can just do  sess.run(simulate_op)  to simulate, if you want to access\nany quantity at any time you can run  sess.run(x.d(0)) , where  x  is the quantity that you want to know the value of.  If you wish want to know more about how this package works, have a look at the example it the  examples folder .   Tip  It is quite handy to define a  t  quantity and set it's first derivative to one in order to keep track of the \"time\". The library does not do this for you.",
            "title": "Example:"
        },
        {
            "location": "/optim/",
            "text": "Optimizing your Simulations\n\n\nVectors\n\n\nTo make your code perform better, try to pack as many quantities as possible into vectors. Especially on GPUs this dramatically \nincreases the performance of your code. \n\n\nExample (Three different scalar quantities):\n\n\n(\nx\n,\ny\n,\nz\n)\n \n=\n \neq\n.\ngenerate_quantities\n(\n3\n)\n\n\neq\n.\nprepare_quantity_for_recursive\n(\nx\n,\n \n4.0\n,\n \n0\n,\n \n1\n)\n\n\neq\n.\nprepare_quantity_for_recursive\n(\ny\n,\n \n2.0\n,\n \n0\n,\n \n1\n)\n\n\neq\n.\nprepare_quantity_for_recursive\n(\nz\n,\n \n3.0\n,\n \n0\n,\n \n1\n)\n\n\neq\n.\ndefine_quantity_recursively\n(\nx\n,\n \nsigma\n \n*\n \n(\ny\n.\nd\n(\n0\n)\n-\nx\n.\nd\n(\n0\n)))\n\n\neq\n.\ndefine_quantity_recursively\n(\ny\n,\n \nx\n.\nd\n(\n0\n)\n*\n(\nrho\n \n-\n \nz\n.\nd\n(\n0\n))\n-\ny\n.\nd\n(\n0\n))\n\n\neq\n.\ndefine_quantity_recursively\n(\nz\n,\n \nx\n.\nd\n(\n0\n)\n*\ny\n.\nd\n(\n0\n)\n \n-\n \nz\n.\nd\n(\n0\n)\n*\nbeta\n)\n\n\n\n\n\n\nExample (One vector quantity):\n\n\n(\nxyz\n,)\n \n=\n \neq\n.\ngenerate_quantities\n(\n1\n)\n\n\neq\n.\ndefine_quantity\n(\nxyz\n,\nnp\n.\narray\n([\n2.1\n,\n \n3.2\n,\n \n4.4\n]),\n \n0\n,\n \n1\n)(\ntf\n.\npack\n([\nsigma\n \n*\n \n(\nxyz\n.\nd\n(\n0\n)[\n1\n]\n-\nxyz\n.\nd\n(\n0\n)[\n0\n]),\n\n                                                                    \nxyz\n.\nd\n(\n0\n)[\n0\n]\n \n*\n \n(\nrho\n \n-\n \nxyz\n.\nd\n(\n0\n)[\n2\n])\n \n-\n \nxyz\n.\nd\n(\n0\n)[\n1\n],\n\n                                                                    \nxyz\n.\nd\n(\n0\n)[\n0\n]\n*\nxyz\n.\nd\n(\n0\n)[\n1\n]\n \n-\n \nbeta\n \n*\n \nxyz\n.\nd\n(\n0\n)[\n2\n]]))\n\n\n\n\n\n\nYou can see the performance increase for yourself by comparing the performance of the two Lorenz Attractor examples, \nthe \nlorenz_attractor_recursive.py\n script uses three different quantities, while \nlorenz_attractor.py\n uses a vector quantity.",
            "title": "Optimizing your Simulations"
        },
        {
            "location": "/optim/#optimizing-your-simulations",
            "text": "",
            "title": "Optimizing your Simulations"
        },
        {
            "location": "/optim/#vectors",
            "text": "To make your code perform better, try to pack as many quantities as possible into vectors. Especially on GPUs this dramatically \nincreases the performance of your code.",
            "title": "Vectors"
        },
        {
            "location": "/optim/#example-three-different-scalar-quantities",
            "text": "( x , y , z )   =   eq . generate_quantities ( 3 )  eq . prepare_quantity_for_recursive ( x ,   4.0 ,   0 ,   1 )  eq . prepare_quantity_for_recursive ( y ,   2.0 ,   0 ,   1 )  eq . prepare_quantity_for_recursive ( z ,   3.0 ,   0 ,   1 )  eq . define_quantity_recursively ( x ,   sigma   *   ( y . d ( 0 ) - x . d ( 0 )))  eq . define_quantity_recursively ( y ,   x . d ( 0 ) * ( rho   -   z . d ( 0 )) - y . d ( 0 ))  eq . define_quantity_recursively ( z ,   x . d ( 0 ) * y . d ( 0 )   -   z . d ( 0 ) * beta )",
            "title": "Example (Three different scalar quantities):"
        },
        {
            "location": "/optim/#example-one-vector-quantity",
            "text": "( xyz ,)   =   eq . generate_quantities ( 1 )  eq . define_quantity ( xyz , np . array ([ 2.1 ,   3.2 ,   4.4 ]),   0 ,   1 )( tf . pack ([ sigma   *   ( xyz . d ( 0 )[ 1 ] - xyz . d ( 0 )[ 0 ]), \n                                                                     xyz . d ( 0 )[ 0 ]   *   ( rho   -   xyz . d ( 0 )[ 2 ])   -   xyz . d ( 0 )[ 1 ], \n                                                                     xyz . d ( 0 )[ 0 ] * xyz . d ( 0 )[ 1 ]   -   beta   *   xyz . d ( 0 )[ 2 ]]))   You can see the performance increase for yourself by comparing the performance of the two Lorenz Attractor examples, \nthe  lorenz_attractor_recursive.py  script uses three different quantities, while  lorenz_attractor.py  uses a vector quantity.",
            "title": "Example (One vector quantity):"
        }
    ]
}